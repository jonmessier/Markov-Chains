# Markov-Chains
Exploration of Markov Chains
--- 
Introduction

The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed. In other words, the probability of transitioning to any particular state is dependent solely on the current state and time elapsed. The state space, or set of all possible states, can be anything: letters, numbers, weather conditions, health conditions or stock performances.

A Markov chain is a stochastic model describing a sequence of states and the probability of transitioning from one state to another. Markov chains have no history and only rely on the current state and the probability to move to another state. Markov chains can be discrete in space or in time over continuous spaces. 

In this notebook I will show my study of Markov chains, and a few examples that can be used to understand the topic
